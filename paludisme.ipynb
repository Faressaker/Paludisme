{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "81d1bdf2-e42d-4a13-9a12-4085ec383d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.applications import VGG16\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c930a0cc-92d1-45db-90a7-0567d9c8fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins des dossiers\n",
    "data_directory = r'C:\\Users\\skrfa\\Desktop/Réseaux_de_neurones/malaria_hematie_dataset'\n",
    "saines_directory = r'C:\\Users\\skrfa\\Desktop/Réseaux_de_neurones/malaria_hematie_dataset/saines'\n",
    "infectees_directory = r'C:\\Users\\skrfa\\Desktop/Réseaux_de_neurones/malaria_hematie_dataset/infectees'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a980a72-5fac-40ba-832d-5539c165d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préprocesser les images\n",
    "def preprocess_images(folder_path, label, target_size=(128, 128)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = load_img(img_path, target_size=target_size)\n",
    "            img_array = img_to_array(img)\n",
    "            images.append(img_array)\n",
    "            labels.append(label)\n",
    "    return np.array(images), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5bfdd408-728c-4bc2-afaf-0f66d6b78137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13779/13779 [00:18<00:00, 761.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 13779/13779 [00:14<00:00, 964.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# préprocesser les images infectees\n",
    "infectees_images, infectees_labels = preprocess_images(r'C:\\Users\\skrfa\\Desktop/Réseaux_de_neurones/malaria_hematie_dataset/infectees', 'infectees')\n",
    "\n",
    "# préprocesserles images saines\n",
    "saines_images, saines_labels = preprocess_images(r'C:\\Users\\skrfa\\Desktop/Réseaux_de_neurones/malaria_hematie_dataset/saines', 'saines')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e999a-207f-4073-a993-3bbbef64b2ff",
   "metadata": {},
   "source": [
    "**Assurer que toutes les images ont la même shape permet de garantir que les données sont prêtes pour l'entraînement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d9f72211-4324-408b-aa28-efedb0be67dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image infectees Shapes: {(128, 128, 3)}\n",
      "Image Saines Shapes: {(128, 128, 3)}\n"
     ]
    }
   ],
   "source": [
    "# shape des images\n",
    "print(\"Image infectees Shapes:\", set(img.shape for img in infectees_images))\n",
    "print(\"Image Saines Shapes:\", set(img.shape for img in saines_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "00e9fe52-b5f8-4792-a112-e8ad848e614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenater les données Preprocesser\n",
    "images = np.concatenate((infectees_images, saines_images))\n",
    "labels = infectees_labels + saines_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "962e53ed-d3c6-4d03-94ac-16410e0fa66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation des images\n",
    "images_normalized = images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "80beeeec-e300-449f-a61b-b34d0abe5cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage des labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70194152-3450-491a-843a-c2a39e9d8f38",
   "metadata": {},
   "source": [
    "**Vérifier les shapes et le nombre des labels est necessaire pour verifier la cohérence des données et detecter les erreurs** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad861708-dddd-4486-944f-7daba1163256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Images: (27558, 128, 128, 3)\n",
      "Length of Labels: 27558\n"
     ]
    }
   ],
   "source": [
    "# Vérifier les shapes et le nombre des labels  \n",
    "print(\"Shape of Images:\", images.shape)\n",
    "print(\"Length of Labels:\", len(labels_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678a067-d7a4-4b91-959c-78c103da5dea",
   "metadata": {},
   "source": [
    "**Data augmentation : est une technique utilisée pour augmenter la diversité de l'ensemble de données d'entraînement sans collecter de nouvelles données et pour reduire le surapprentissage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "78d1ea3a-f8be-473c-bbd9-f5057d12942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation des données\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125de7ce-d75f-4f57-bd79-d5c8be2199a8",
   "metadata": {},
   "source": [
    "**Definir les dimensions des images et créer un modèle CNN en utilisant Keras pour la classification des images d'hématies infectées et saines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "99199b9a-94ed-49c6-81d0-c1d98eb1580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détermination des dimensions des images prétraitées ==> Récupère les dimensions des images prétraitées (hauteur, largeur)\n",
    "img_height, img_width, channels = images.shape[1:]\n",
    "\n",
    "# Détermination du nombre de classes uniques ==> Détermine le nombre de classes uniques dans les labels encodés\n",
    "num_classes = len(np.unique(labels_encoded))\n",
    "\n",
    "# Construction du modèle CNN \n",
    "model = Sequential([\n",
    "    # Première couche de convolution et pooling\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, channels)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Deuxième couche de convolution et pooling\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flattening\n",
    "    Flatten(),\n",
    "    \n",
    "    # Couche dense avec dropout\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Couche de sortie\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "09209c58-ef01-4398-bb64-d7ba04256c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle VGG16\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# Récuperation de l'encodeur pré-entraîné\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Construction du modèle et ajouter des couches Dense\n",
    "x = Flatten()(base_model.output)  # Flatten pour connecter à des couches Dense\n",
    "x = Dense(128, activation='relu')(x)  # Couche Dense avec 128 neurones et activation ReLU\n",
    "x = Dropout(0.5)(x)  # Dropout de 50% pour la régularisation\n",
    "predictions = Dense(num_classes, activation='softmax')(x)  # Couche de sortie avec activation softmax\n",
    "\n",
    "# Création du modèle final avec l'encodeur pré-entraîné et les nouvelles couches\n",
    "model_vgg = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4a3f92de-0af8-40b8-a1b0-0c3b4173f4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 209ms/step - accuracy: 0.7109 - loss: 0.6009 - val_accuracy: 0.9652 - val_loss: 0.2866 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 210ms/step - accuracy: 0.9298 - loss: 0.2269 - val_accuracy: 0.7658 - val_loss: 0.4715 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 214ms/step - accuracy: 0.9330 - loss: 0.1991 - val_accuracy: 0.9048 - val_loss: 0.2565 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 210ms/step - accuracy: 0.9512 - loss: 0.1492 - val_accuracy: 0.9178 - val_loss: 0.2585 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 190ms/step - accuracy: 0.9584 - loss: 0.1276 - val_accuracy: 0.9006 - val_loss: 0.2985 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 187ms/step - accuracy: 0.9693 - loss: 0.0970 - val_accuracy: 0.9260 - val_loss: 0.2234 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 191ms/step - accuracy: 0.9725 - loss: 0.0835 - val_accuracy: 0.9338 - val_loss: 0.2092 - learning_rate: 1.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 193ms/step - accuracy: 0.9716 - loss: 0.0774 - val_accuracy: 0.9320 - val_loss: 0.2212 - learning_rate: 1.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 184ms/step - accuracy: 0.9724 - loss: 0.0771 - val_accuracy: 0.9354 - val_loss: 0.2098 - learning_rate: 1.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 186ms/step - accuracy: 0.9728 - loss: 0.0708 - val_accuracy: 0.9285 - val_loss: 0.2269 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17e21291950>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Définition des callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=3, monitor='val_loss'),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=2, monitor='val_loss')\n",
    "]\n",
    "\n",
    "# Compilation et entraînement des modèles\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(images_normalized, labels_encoded, epochs=10, validation_split=0.2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0e2b20dd-5146-4646-8bf1-3eb571f33e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des poids des modèles\n",
    "model.save_weights('model_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0dbf9f-47c6-47a7-98f3-d381f84350c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
